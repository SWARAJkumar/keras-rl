{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SWARAJ\\Desktop\\keras-rl-master\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    31/50000: episode: 1, duration: 8.982s, episode steps: 31, steps per second: 3, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.013 [-1.185, 1.776], loss: 0.461123, mean_absolute_error: 0.519512, mean_q: 0.094664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SWARAJ\\Desktop\\keras-rl-master\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    44/50000: episode: 2, duration: 0.341s, episode steps: 13, steps per second: 38, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.069 [-1.867, 1.224], loss: 0.354990, mean_absolute_error: 0.543738, mean_q: 0.286587\n",
      "    64/50000: episode: 3, duration: 0.499s, episode steps: 20, steps per second: 40, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.070 [-1.375, 0.833], loss: 0.226187, mean_absolute_error: 0.557896, mean_q: 0.506910\n",
      "    83/50000: episode: 4, duration: 0.479s, episode steps: 19, steps per second: 40, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.074 [-0.817, 1.505], loss: 0.115400, mean_absolute_error: 0.604359, mean_q: 0.817629\n",
      "    98/50000: episode: 5, duration: 0.367s, episode steps: 15, steps per second: 41, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.090 [-1.368, 0.825], loss: 0.055641, mean_absolute_error: 0.679505, mean_q: 1.145108\n",
      "   114/50000: episode: 6, duration: 0.402s, episode steps: 16, steps per second: 40, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.072 [-3.056, 1.962], loss: 0.032638, mean_absolute_error: 0.702948, mean_q: 1.268106\n",
      "   128/50000: episode: 7, duration: 0.350s, episode steps: 14, steps per second: 40, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.111 [-0.934, 1.483], loss: 0.040802, mean_absolute_error: 0.761719, mean_q: 1.437962\n",
      "   151/50000: episode: 8, duration: 0.586s, episode steps: 23, steps per second: 39, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.050 [-1.760, 2.679], loss: 0.026185, mean_absolute_error: 0.819899, mean_q: 1.600839\n",
      "   162/50000: episode: 9, duration: 0.286s, episode steps: 11, steps per second: 38, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.133 [-1.133, 1.962], loss: 0.034294, mean_absolute_error: 0.887385, mean_q: 1.749085\n",
      "   175/50000: episode: 10, duration: 0.328s, episode steps: 13, steps per second: 40, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.118 [-2.439, 1.549], loss: 0.051733, mean_absolute_error: 0.930162, mean_q: 1.823170\n",
      "   208/50000: episode: 11, duration: 0.825s, episode steps: 33, steps per second: 40, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.394 [0.000, 1.000], mean observation: 0.009 [-1.580, 2.195], loss: 0.040077, mean_absolute_error: 1.005681, mean_q: 2.010506\n",
      "   223/50000: episode: 12, duration: 0.368s, episode steps: 15, steps per second: 41, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.096 [-1.845, 1.032], loss: 0.040337, mean_absolute_error: 1.089134, mean_q: 2.214713\n",
      "   235/50000: episode: 13, duration: 0.310s, episode steps: 12, steps per second: 39, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.124 [-1.329, 2.154], loss: 0.072917, mean_absolute_error: 1.162602, mean_q: 2.297668\n",
      "   249/50000: episode: 14, duration: 0.338s, episode steps: 14, steps per second: 41, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.103 [-0.965, 1.601], loss: 0.047723, mean_absolute_error: 1.210876, mean_q: 2.503114\n",
      "   264/50000: episode: 15, duration: 0.379s, episode steps: 15, steps per second: 40, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.086 [-1.187, 2.013], loss: 0.085710, mean_absolute_error: 1.299662, mean_q: 2.576329\n",
      "   289/50000: episode: 16, duration: 0.625s, episode steps: 25, steps per second: 40, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.054 [-1.365, 2.263], loss: 0.096606, mean_absolute_error: 1.379424, mean_q: 2.715918\n",
      "   334/50000: episode: 17, duration: 1.143s, episode steps: 45, steps per second: 39, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: -0.108 [-2.077, 2.037], loss: 0.095733, mean_absolute_error: 1.527286, mean_q: 3.034200\n",
      "   346/50000: episode: 18, duration: 0.299s, episode steps: 12, steps per second: 40, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.112 [-1.578, 2.518], loss: 0.121381, mean_absolute_error: 1.695776, mean_q: 3.360375\n",
      "   384/50000: episode: 19, duration: 0.969s, episode steps: 38, steps per second: 39, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.065 [-1.135, 0.550], loss: 0.106861, mean_absolute_error: 1.778552, mean_q: 3.508057\n",
      "   406/50000: episode: 20, duration: 0.555s, episode steps: 22, steps per second: 40, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.036 [-2.278, 1.562], loss: 0.129026, mean_absolute_error: 1.907096, mean_q: 3.766783\n",
      "   418/50000: episode: 21, duration: 0.298s, episode steps: 12, steps per second: 40, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.135 [-0.961, 1.649], loss: 0.216250, mean_absolute_error: 1.976415, mean_q: 3.761609\n",
      "   431/50000: episode: 22, duration: 0.323s, episode steps: 13, steps per second: 40, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.109 [-1.383, 2.208], loss: 0.234754, mean_absolute_error: 2.039302, mean_q: 3.864477\n",
      "   444/50000: episode: 23, duration: 0.329s, episode steps: 13, steps per second: 40, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.083 [-2.147, 1.406], loss: 0.171753, mean_absolute_error: 2.093499, mean_q: 4.063066\n",
      "   460/50000: episode: 24, duration: 0.394s, episode steps: 16, steps per second: 41, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.100 [-0.586, 1.215], loss: 0.149745, mean_absolute_error: 2.131176, mean_q: 4.176792\n",
      "   484/50000: episode: 25, duration: 0.601s, episode steps: 24, steps per second: 40, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.040 [-0.805, 1.402], loss: 0.179472, mean_absolute_error: 2.254635, mean_q: 4.375271\n",
      "   497/50000: episode: 26, duration: 0.325s, episode steps: 13, steps per second: 40, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.105 [-0.766, 1.234], loss: 0.220927, mean_absolute_error: 2.312657, mean_q: 4.399607\n",
      "   515/50000: episode: 27, duration: 0.447s, episode steps: 18, steps per second: 40, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.046 [-1.377, 2.033], loss: 0.210924, mean_absolute_error: 2.394525, mean_q: 4.591862\n",
      "   557/50000: episode: 28, duration: 1.060s, episode steps: 42, steps per second: 40, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: -0.024 [-1.985, 2.731], loss: 0.207842, mean_absolute_error: 2.492649, mean_q: 4.787884\n",
      "   576/50000: episode: 29, duration: 0.481s, episode steps: 19, steps per second: 39, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.109 [-0.543, 1.031], loss: 0.156249, mean_absolute_error: 2.626363, mean_q: 5.075217\n",
      "   647/50000: episode: 30, duration: 1.785s, episode steps: 71, steps per second: 40, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.042 [-0.769, 1.092], loss: 0.231019, mean_absolute_error: 2.788114, mean_q: 5.336424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   672/50000: episode: 31, duration: 0.752s, episode steps: 25, steps per second: 33, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.039 [-1.018, 1.734], loss: 0.206568, mean_absolute_error: 2.937283, mean_q: 5.630044\n",
      "   684/50000: episode: 32, duration: 0.515s, episode steps: 12, steps per second: 23, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.102 [-1.031, 1.762], loss: 0.151466, mean_absolute_error: 3.016392, mean_q: 5.824205\n",
      "   697/50000: episode: 33, duration: 0.533s, episode steps: 13, steps per second: 24, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.105 [-0.956, 1.545], loss: 0.300855, mean_absolute_error: 3.065390, mean_q: 5.822746\n",
      "   707/50000: episode: 34, duration: 0.369s, episode steps: 10, steps per second: 27, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.135 [-1.123, 1.844], loss: 0.222009, mean_absolute_error: 3.100207, mean_q: 5.987142\n",
      "   731/50000: episode: 35, duration: 1.050s, episode steps: 24, steps per second: 23, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.070 [-1.300, 0.755], loss: 0.176493, mean_absolute_error: 3.163780, mean_q: 6.141502\n",
      "   756/50000: episode: 36, duration: 0.793s, episode steps: 25, steps per second: 32, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.036 [-1.197, 1.881], loss: 0.278058, mean_absolute_error: 3.278249, mean_q: 6.323355\n",
      "   836/50000: episode: 37, duration: 2.677s, episode steps: 80, steps per second: 30, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.864, 1.788], loss: 0.224341, mean_absolute_error: 3.444926, mean_q: 6.683314\n",
      "   879/50000: episode: 38, duration: 1.696s, episode steps: 43, steps per second: 25, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.097 [-0.450, 1.106], loss: 0.231394, mean_absolute_error: 3.677132, mean_q: 7.174236\n"
     ]
    }
   ],
   "source": [
    "%run examples/dqn_cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
